/**
 * @typedef {import('./utils/hub.js').PretrainedOptions} PretrainedOptions
 */
/**
 * Utility factory method to build a [`Pipeline`] object.
 *
 * @param {string} task The task of the pipeline.
 * @param {string} [model=null] The name of the pre-trained model to use. If not specified, the default model for the task will be used.
 * @param {PretrainedOptions} [options] Optional parameters for the pipeline.
 * @returns {Promise<Pipeline>} A Pipeline object for the specified task.
 * @throws {Error} If an unsupported pipeline is requested.
 */
export function pipeline(task: string, model?: string, { quantized, progress_callback, config, cache_dir, local_files_only, revision, }?: PretrainedOptions): Promise<Pipeline>;
declare const Pipeline_base: new () => {
    (...args: any[]): any;
    _call(...args: any[]): any;
};
/**
 * The Pipeline class is the class from which all pipelines inherit.
 * Refer to this class for methods shared across different pipelines.
 * @extends Callable
 */
export class Pipeline extends Pipeline_base {
    /**
     * Create a new Pipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedTokenizer} tokenizer The tokenizer to use.
     * @param {PreTrainedModel} model The model to use.
     */
    constructor(task: string, tokenizer: PreTrainedTokenizer, model: PreTrainedModel);
    task: string;
    tokenizer: PreTrainedTokenizer;
    model: PreTrainedModel;
    /**
     * Disposes the model.
     * @returns {Promise<void>} A promise that resolves when the model has been disposed.
     */
    dispose(): Promise<void>;
    /**
     * Executes the task associated with the pipeline.
     * @param {any} texts The input texts to be processed.
     * @returns {Promise<any>} A promise that resolves to an array containing the inputs and outputs of the task.
     */
    _call(texts: any): Promise<any>;
}
/**
 * Text classification pipeline using any `ModelForSequenceClassification`.
 * @extends Pipeline
 */
export class TextClassificationPipeline extends Pipeline {
    /**
     * Executes the text classification task.
     * @param {any} texts The input texts to be classified.
     * @param {Object} options An optional object containing the following properties:
     * @param {number} [options.topk=1] The number of top predictions to be returned.
     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted labels and scores.
     */
    _call(texts: any, { topk }?: {
        topk?: number;
    }): Promise<any[] | any>;
}
/**
 * Named Entity Recognition pipeline using any `ModelForTokenClassification`.
 * @extends Pipeline
 */
export class TokenClassificationPipeline extends Pipeline {
    /**
     * Executes the token classification task.
     * @param {any} texts The input texts to be classified.
     * @param {Object} options An optional object containing the following properties:
     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted labels and scores.
     */
    _call(texts: any, { ignore_labels, }?: any): Promise<any[] | any>;
}
/**
 * Question Answering pipeline using any `ModelForQuestionAnswering`.
 *
 * **Example:** Run question answering with `distilbert-base-uncased-distilled-squad`.
 * ```javascript
 * let question = 'Who was Jim Henson?';
 * let context = 'Jim Henson was a nice puppet.';
 *
 * let answerer = await pipeline('question-answering', 'Xenova/distilbert-base-uncased-distilled-squad');
 * let outputs = await answerer(question, context);
 * console.log(outputs);
 * // {
 * //     "answer": "a nice puppet",
 * //     "score": 0.5768911502526741
 * // }
 * ```
 * @extends Pipeline
 */
export class QuestionAnsweringPipeline extends Pipeline {
    /**
     * Executes the question answering task.
     * @param {string|string[]} question The question(s) to be answered.
     * @param {string|string[]} context The context(s) where the answer(s) can be found.
     * @param {Object} options An optional object containing the following properties:
     * @param {number} [options.topk=1] The number of top answer predictions to be returned.
     * @returns {Promise<any>} A promise that resolves to an array or object containing the predicted answers and scores.
     */
    _call(question: string | string[], context: string | string[], { topk }?: {
        topk?: number;
    }): Promise<any>;
}
/**
 * Masked language modeling prediction pipeline using any `ModelWithLMHead`.
 * @extends Pipeline
 */
export class FillMaskPipeline extends Pipeline {
    /**
     * Fill the masked token in the text(s) given as inputs.
     * @param {any} texts The masked input texts.
     * @param {Object} options An optional object containing the following properties:
     * @param {number} [options.topk=5] The number of top predictions to be returned.
     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted tokens and scores.
     */
    _call(texts: any, { topk }?: {
        topk?: number;
    }): Promise<any[] | any>;
}
/**
 * Text2TextGenerationPipeline class for generating text using a model that performs text-to-text generation tasks.
 * @extends Pipeline
 */
export class Text2TextGenerationPipeline extends Pipeline {
    _key: any;
    /**
     * Fill the masked token in the text(s) given as inputs.
     * @param {string|string[]} texts The text or array of texts to be processed.
     * @param {Object} [options={}] Options for the fill-mask pipeline.
     * @param {number} [options.topk=5] The number of top-k predictions to return.
     * @returns {Promise<any>} An array of objects containing the score, predicted token, predicted token string,
     * and the sequence with the predicted token filled in, or an array of such arrays (one for each input text).
     * If only one input text is given, the output will be an array of objects.
     * @throws {Error} When the mask token is not found in the input text.
     */
    _call(texts: string | string[], generate_kwargs?: {}): Promise<any>;
}
/**
 * A pipeline for summarization tasks, inheriting from Text2TextGenerationPipeline.
 * @extends Text2TextGenerationPipeline
 */
export class SummarizationPipeline extends Text2TextGenerationPipeline {
    _key: string;
}
/**
 * TranslationPipeline class to translate text from one language to another using the provided model and tokenizer.
 * @extends Text2TextGenerationPipeline
 */
export class TranslationPipeline extends Text2TextGenerationPipeline {
    _key: string;
}
/**
 * Language generation pipeline using any `ModelWithLMHead` or `ModelForCausalLM`.
 * This pipeline predicts the words that will follow a specified text prompt.
 * NOTE: For the full list of generation parameters, see [`GenerationConfig`](./utils/generation#module_utils/generation.GenerationConfig).
 *
 * **Example:** Text generation with `Xenova/distilgpt2` (default settings).
 * ```javascript
 * let text = 'I enjoy walking with my cute dog,';
 * let generator = await pipeline('text-generation', 'Xenova/distilgpt2');
 * let output = await generator(text);
 * console.log(output);
 * // [{ generated_text: "I enjoy walking with my cute dog, and I love to play with the other dogs." }]
 * ```
 *
 * **Example:** Text generation with `Xenova/distilgpt2` (custom settings).
 * ```javascript
 * let text = 'Once upon a time, there was';
 * let generator = await pipeline('text-generation', 'Xenova/distilgpt2');
 * let output = await generator(text, {
 *     temperature: 2,
 *     max_new_tokens: 10,
 *     repetition_penalty: 1.5,
 *     no_repeat_ngram_size: 2,
 *     num_beams: 2,
 *     num_return_sequences: 2,
 * });
 * console.log(output);
 * // [{
 * //   "generated_text": "Once upon a time, there was an abundance of information about the history and activities that"
 * // }, {
 * //   "generated_text": "Once upon a time, there was an abundance of information about the most important and influential"
 * // }]
 * ```
 *
 * **Example:** Run code generation with `Xenova/codegen-350M-mono`.
 * ```javascript
 * let text = 'def fib(n):';
 * let generator = await pipeline('text-generation', 'Xenova/codegen-350M-mono');
 * let output = await generator(text, {
 *     max_new_tokens: 40,
 * });
 * console.log(output[0].generated_text);
 * // def fib(n):
 * //     if n == 0:
 * //         return 0
 * //     if n == 1:
 * //         return 1
 * //     return fib(n-1) + fib(n-2)
 * ```
 *
 * @extends Pipeline
 */
export class TextGenerationPipeline extends Pipeline {
    /**
     * Generates text based on an input prompt.
     * @param {any} texts The input prompt or prompts to generate text from.
     * @param {Object} [generate_kwargs={}] Additional arguments for text generation.
     * @returns {Promise<any>} The generated text or texts.
     */
    _call(texts: any, generate_kwargs?: any): Promise<any>;
}
/**
 * NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification`
 * trained on NLI (natural language inference) tasks. Equivalent of `text-classification`
 * pipelines, but these models don't require a hardcoded number of potential classes, they
 * can be chosen at runtime. It usually means it's slower but it is **much** more flexible.
 * @extends Pipeline
 */
export class ZeroShotClassificationPipeline extends Pipeline {
    label2id: {
        [k: string]: any;
    };
    entailment_id: any;
    contradiction_id: any;
    /**
     * @param {any[]} texts
     * @param {string[]} candidate_labels
     * @param {Object} options Additional options:
     * @param {string} [options.hypothesis_template="This example is {}."] The template used to turn each
     * candidate label into an NLI-style hypothesis. The candidate label will replace the {} placeholder.
     * @param {boolean} [options.multi_label=false] Whether or not multiple candidate labels can be true.
     * If `false`, the scores are normalized such that the sum of the label likelihoods for each sequence
     * is 1. If `true`, the labels are considered independent and probabilities are normalized for each
     * candidate by doing a softmax of the entailment score vs. the contradiction score.
     * @return {Promise<Object|Object[]>} The prediction(s), as a map (or list of maps) from label to score.
     */
    _call(texts: any[], candidate_labels: string[], { hypothesis_template, multi_label, }?: {
        hypothesis_template?: string;
        multi_label?: boolean;
    }): Promise<any | any[]>;
}
/**
 * Feature extraction pipeline using no model head. This pipeline extracts the hidden
 * states from the base transformer, which can be used as features in downstream tasks.
 *
 * **Example:** Run feature extraction with `bert-base-uncased` (without pooling/normalization).
 * ```javascript
 * let extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });
 * let result = await extractor('This is a simple test.');
 * console.log(result);
 * // Tensor {
 * //     type: 'float32',
 * //     data: Float32Array [0.05939924716949463, 0.021655935794115067, ...],
 * //     dims: [1, 8, 768]
 * // }
 * ```
 *
 * **Example:** Run feature extraction with `bert-base-uncased` (with pooling/normalization).
 * ```javascript
 * let extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });
 * let result = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });
 * console.log(result);
 * // Tensor {
 * //     type: 'float32',
 * //     data: Float32Array [0.03373778983950615, -0.010106077417731285, ...],
 * //     dims: [1, 768]
 * // }
 * ```
 *
 * **Example:** Calculating embeddings with `sentence-transformers` models.
 * ```javascript
 * let extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
 * let result = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });
 * console.log(result);
 * // Tensor {
 * //     type: 'float32',
 * //     data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],
 * //     dims: [1, 384]
 * // }
 * ```
 * @extends Pipeline
 */
export class FeatureExtractionPipeline extends Pipeline {
    /**
     * Extract the features of the input(s).
     *
     * @param {string|string[]} texts The input texts
     * @param {Object} options Additional options:
     * @param {string} [options.pooling="none"] The pooling method to use. Can be one of: "none", "mean".
     * @param {boolean} [options.normalize=false] Whether or not to normalize the embeddings in the last dimension.
     * @returns The features computed by the model.
     */
    _call(texts: string | string[], { pooling, normalize, }?: {
        pooling?: string;
        normalize?: boolean;
    }): Promise<any>;
}
/**
 * Pipeline that aims at extracting spoken text contained within some audio.
 *
 * **Example:** Transcribe English.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
 * let output = await transcriber(url);
 * // { text: " And so my fellow Americans ask not what your country can do for you, ask what you can do for your country." }
 * ```
 *
 * **Example:** Transcribe English w/ timestamps.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
 * let output = await transcriber(url, { return_timestamps: true });
 * // {
 * //   text: " And so my fellow Americans ask not what your country can do for you, ask what you can do for your country."
 * //   chunks: [
 * //     { timestamp: [0, 8],  text: " And so my fellow Americans ask not what your country can do for you" }
 * //     { timestamp: [8, 11], text: " ask what you can do for your country." }
 * //   ]
 * // }
 * ```
 *
 * **Example:** Transcribe English w/ word-level timestamps.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', {
 *     revision: 'output_attentions',
 * });
 * let output = await transcriber(url, { return_timestamps: 'word' });
 * // {
 * //   "text": " And so my fellow Americans ask not what your country can do for you ask what you can do for your country.",
 * //   "chunks": [
 * //     { "text": " And", "timestamp": [0, 0.78] },
 * //     { "text": " so", "timestamp": [0.78, 1.06] },
 * //     { "text": " my", "timestamp": [1.06, 1.46] },
 * //     ...
 * //     { "text": " for", "timestamp": [9.72, 9.92] },
 * //     { "text": " your", "timestamp": [9.92, 10.22] },
 * //     { "text": " country.", "timestamp": [10.22, 13.5] }
 * //   ]
 * // }
 * ```
 *
 * **Example:** Transcribe French.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/french-audio.mp3';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');
 * let output = await transcriber(url, { language: 'french', task: 'transcribe' });
 * // { text: " J'adore, j'aime, je n'aime pas, je d√©teste." }
 * ```
 *
 * **Example:** Translate French to English.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/french-audio.mp3';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');
 * let output = await transcriber(url, { language: 'french', task: 'translate' });
 * // { text: " I love, I like, I don't like, I hate." }
 * ```
 *
 * **Example:** Transcribe/translate audio longer than 30 seconds.
 * ```javascript
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/ted_60.wav';
 * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
 * let output = await transcriber(url, { chunk_length_s: 30, stride_length_s: 5 });
 * // { text: " So in college, I was a government major, which means [...] So I'd start off light and I'd bump it up" }
 * ```
 * @extends Pipeline
 */
export class AutomaticSpeechRecognitionPipeline extends Pipeline {
    /**
     * Create a new AutomaticSpeechRecognitionPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedTokenizer} tokenizer The tokenizer to use.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, tokenizer: PreTrainedTokenizer, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    /**
     * Preprocesses the input audio for the AutomaticSpeechRecognitionPipeline.
     * @param {any} audio The audio to be preprocessed.
     * @param {number} sampling_rate The sampling rate of the audio.
     * @returns {Promise<Float32Array>} A promise that resolves to the preprocessed audio data.
     * @private
     */
    private _preprocess;
    /**
     * @typedef {import('./utils/tensor.js').Tensor} Tensor
     * @typedef {{stride: number[], input_features: Tensor, is_last: boolean, tokens?: number[], token_timestamps?: number[]}} Chunk
     *
     * @callback ChunkCallback
     * @param {Chunk} chunk The chunk to process.
     */
    /**
     * Asynchronously processes audio and generates text transcription using the model.
     * @param {Float32Array|Float32Array[]} audio The audio to be transcribed. Can be a single Float32Array or an array of Float32Arrays.
     * @param {Object} [kwargs={}] Optional arguments.
     * @param {boolean|'word'} [kwargs.return_timestamps] Whether to return timestamps or not. Default is `false`.
     * @param {number} [kwargs.chunk_length_s] The length of audio chunks to process in seconds. Default is 0 (no chunking).
     * @param {number} [kwargs.stride_length_s] The length of overlap between consecutive audio chunks in seconds. If not provided, defaults to `chunk_length_s / 6`.
     * @param {ChunkCallback} [kwargs.chunk_callback] Callback function to be called with each chunk processed.
     * @param {boolean} [kwargs.force_full_sequences] Whether to force outputting full sequences or not. Default is `false`.
     * @param {string} [kwargs.language] The source language. Default is `null`, meaning it should be auto-detected. Use this to potentially improve performance if the source language is known.
     * @param {string} [kwargs.task] The task to perform. Default is `null`, meaning it should be auto-detected.
     * @param {number[][]} [kwargs.forced_decoder_ids] A list of pairs of integers which indicates a mapping from generation indices to token indices
     * that will be forced before sampling. For example, [[1, 123]] means the second generated token will always be a token of index 123.
     * @returns {Promise<Object>} A Promise that resolves to an object containing the transcription text and optionally timestamps if `return_timestamps` is `true`.
     */
    _call(audio: Float32Array | Float32Array[], kwargs?: {
        return_timestamps?: boolean | 'word';
        chunk_length_s?: number;
        stride_length_s?: number;
        chunk_callback?: (chunk: {
            stride: number[];
            input_features: import("./utils/tensor.js").Tensor;
            is_last: boolean;
            tokens?: number[];
            token_timestamps?: number[];
        }) => any;
        force_full_sequences?: boolean;
        language?: string;
        task?: string;
        forced_decoder_ids?: number[][];
    }): Promise<any>;
}
/**
 * Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline predicts a caption for a given image.
 * @extends Pipeline
 */
export class ImageToTextPipeline extends Pipeline {
    /**
     * Create a new ImageToTextPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedTokenizer} tokenizer The tokenizer to use.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, tokenizer: PreTrainedTokenizer, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    /**
     * Assign labels to the image(s) passed as inputs.
     * @param {any[]} images The images to be captioned.
     * @param {Object} [generate_kwargs={}] Optional generation arguments.
     * @returns {Promise<Object|Object[]>} A Promise that resolves to an object (or array of objects) containing the generated text(s).
     */
    _call(images: any[], generate_kwargs?: any): Promise<any | any[]>;
}
/**
 * Image classification pipeline using any `AutoModelForImageClassification`.
 * This pipeline predicts the class of an image.
 *
 * **Example:** Classify an image.
 * ```javascript
 * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';
 * let outputs = await classifier(url);
 * // Array(1) [
 * //   {label: 'tiger, Panthera tigris', score: 0.632695734500885},
 * // ]
 * ```
 *
 * **Example:** Classify an image and return top `n` classes.
 * ```javascript
 * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';
 * let outputs = await classifier(url, { topk: 3 });
 * // Array(3) [
 * //   {label: 'tiger, Panthera tigris', score: 0.632695734500885},
 * //   {label: 'tiger cat', score: 0.3634825646877289},
 * //   {label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707},
 * // ]
 * ```
 *
 * **Example:** Classify an image and return all classes.
 * ```javascript
 * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
 * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';
 * let outputs = await classifier(url, { topk: 0 });
 * // Array(1000) [
 * //   {label: 'tiger, Panthera tigris', score: 0.632695734500885},
 * //   {label: 'tiger cat', score: 0.3634825646877289},
 * //   {label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707},
 * //   {label: 'jaguar, panther, Panthera onca, Felis onca', score: 0.00035465499968267977},
 * //   ...
 * // ]
 * ```
 * @extends Pipeline
 */
export class ImageClassificationPipeline extends Pipeline {
    /**
     * Create a new ImageClassificationPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    /**
     * Classify the given images.
     * @param {any} images The images to classify.
     * @param {Object} options The options to use for classification.
     * @param {number} [options.topk=1] The number of top results to return.
     * @returns {Promise<any>} The top classification results for the images.
     */
    _call(images: any, { topk }?: {
        topk?: number;
    }): Promise<any>;
}
/**
 * Image segmentation pipeline using any `AutoModelForXXXSegmentation`.
 * This pipeline predicts masks of objects and their classes.
 * @extends Pipeline
 */
export class ImageSegmentationPipeline extends Pipeline {
    /**
     * Create a new ImageSegmentationPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    subtasks_mapping: {
        panoptic: string;
        instance: string;
        semantic: string;
    };
    /**
     * Segment the input images.
     * @param {Array} images The input images.
     * @param {Object} options The options to use for segmentation.
     * @param {number} [options.threshold=0.5] Probability threshold to filter out predicted masks.
     * @param {number} [options.mask_threshold=0.5] Threshold to use when turning the predicted masks into binary values.
     * @param {number} [options.overlap_mask_area_threshold=0.8] Mask overlap threshold to eliminate small, disconnected segments.
     * @param {null|string} [options.subtask=null] Segmentation task to be performed. One of [`panoptic`, `instance`, and `semantic`], depending on model capabilities. If not set, the pipeline will attempt to resolve (in that order).
     * @param {Array} [options.label_ids_to_fuse=null] List of label ids to fuse. If not set, do not fuse any labels.
     * @param {Array} [options.target_sizes=null] List of target sizes for the input images. If not set, use the original image sizes.
     * @returns {Promise<Array>} The annotated segments.
     */
    _call(images: any[], { threshold, mask_threshold, overlap_mask_area_threshold, label_ids_to_fuse, target_sizes, subtask, }?: {
        threshold?: number;
        mask_threshold?: number;
        overlap_mask_area_threshold?: number;
        subtask?: null | string;
        label_ids_to_fuse?: any[];
        target_sizes?: any[];
    }): Promise<any[]>;
}
/**
 * Zero shot image classification pipeline. This pipeline predicts the class of
 * an image when you provide an image and a set of `candidate_labels`.
 * @extends Pipeline
 */
export class ZeroShotImageClassificationPipeline extends Pipeline {
    /**
     * Create a new ZeroShotImageClassificationPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedTokenizer} tokenizer The tokenizer to use.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, tokenizer: PreTrainedTokenizer, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    /**
     * Classify the input images with candidate labels using a zero-shot approach.
     * @param {Array} images The input images.
     * @param {Array} candidate_labels The candidate labels.
     * @param {Object} options The options for the classification.
     * @param {string} [options.hypothesis_template] The hypothesis template to use for zero-shot classification. Default: "This is a photo of {}".
     * @returns {Promise<any>} An array of classifications for each input image or a single classification object if only one input image is provided.
     */
    _call(images: any[], candidate_labels: any[], { hypothesis_template }?: {
        hypothesis_template?: string;
    }): Promise<any>;
}
/**
 * Object detection pipeline using any `AutoModelForObjectDetection`.
 * This pipeline predicts bounding boxes of objects and their classes.
 *
 * **Example:** Run object-detection with `facebook/detr-resnet-50`.
 * ```javascript
 * let img = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg';
 *
 * let detector = await pipeline('object-detection', 'Xenova/detr-resnet-50');
 * let output = await detector(img, { threshold: 0.9 });
 * // [{
 * //   "score": 0.9976370930671692,
 * //   "label": "remote",
 * //   "box": { "xmin": 31, "ymin": 68, "xmax": 190, "ymax": 118 }
 * // },
 * // ...
 * // {
 * //   "score": 0.9984092116355896,
 * //   "label": "cat",
 * //   "box": { "xmin": 331, "ymin": 19, "xmax": 649, "ymax": 371 }
 * // }]
 * ```
 *
 * @extends Pipeline
 */
export class ObjectDetectionPipeline extends Pipeline {
    /**
     * Create a new ObjectDetectionPipeline.
     * @param {string} task The task of the pipeline. Useful for specifying subtasks.
     * @param {PreTrainedModel} model The model to use.
     * @param {Processor} processor The processor to use.
     */
    constructor(task: string, model: PreTrainedModel, processor: Processor);
    processor: Processor;
    /**
     * Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
     * @param {any[]} images The input images.
     * @param {Object} options The options for the object detection.
     * @param {number} [options.threshold=0.9] The threshold used to filter boxes by score.
     * @param {boolean} [options.percentage=false] Whether to return the boxes coordinates in percentage (true) or in pixels (false).
     */
    _call(images: any[], { threshold, percentage, }?: {
        threshold?: number;
        percentage?: boolean;
    }): Promise<any>;
    /**
     * Helper function to convert list [xmin, xmax, ymin, ymax] into object { "xmin": xmin, ... }
     * @param {number[]} box The bounding box as a list.
     * @param {boolean} asInteger Whether to cast to integers.
     * @returns {Object} The bounding box as an object.
     * @private
     */
    private _get_bounding_box;
}
export type PretrainedOptions = import('./utils/hub.js').PretrainedOptions;
import { PreTrainedTokenizer } from './tokenizers.js';
import { PreTrainedModel } from './models.js';
import { Processor } from './processors.js';
export {};
//# sourceMappingURL=pipelines.d.ts.map